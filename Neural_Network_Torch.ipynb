{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ewb9CHdxWnSP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Bank_Personal_Loan_Modelling.csv\")"
      ],
      "metadata": {
        "id": "QTreZSSfWzR7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"ID\", \"ZIP Code\"], axis=1)\n",
        "df = pd.get_dummies(df, columns=[\"Family\", \"Education\", \"Securities Account\", \"CD Account\", \"Online\", \"CreditCard\"])"
      ],
      "metadata": {
        "id": "AWjRtDX5Wzkg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop([\"Personal Loan\"], axis=1).values\n",
        "Y = df[\"Personal Loan\"].values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "aMQyzvGlWzoU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = (X - X.mean()) / X.std()"
      ],
      "metadata": {
        "id": "sJDzFj4bWzw2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "8axUk0nxXHd_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.Tensor(X_train)\n",
        "Y_train = torch.Tensor(Y_train)\n",
        "X_test = torch.Tensor(X_test)\n",
        "Y_test = torch.Tensor(Y_test)\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "hidden_dim = 10\n",
        "output_dim = 1\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_dim, hidden_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_dim, output_dim),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_iterations = 100000\n",
        "for i in range(num_iterations):\n",
        "    Y_pred = model(X_train)\n",
        "    loss = criterion(Y_pred, Y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(f\"Iteration {i}: Loss={loss.item()}\")\n",
        "\n",
        "Y_pred = model(X_test)\n",
        "Y_pred_binary = torch.round(Y_pred)\n",
        "acc = accuracy_score(Y_test.detach().numpy(), Y_pred_binary.detach().numpy())\n",
        "print(f\"Accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaUtX8O3ewMy",
        "outputId": "0fa59e44-548f-45d5-e4a4-aa97769616c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Loss=0.568830668926239\n",
            "Iteration 1000: Loss=0.26758909225463867\n",
            "Iteration 2000: Loss=0.22156408429145813\n",
            "Iteration 3000: Loss=0.20843183994293213\n",
            "Iteration 4000: Loss=0.20116564631462097\n",
            "Iteration 5000: Loss=0.19650670886039734\n",
            "Iteration 6000: Loss=0.19366303086280823\n",
            "Iteration 7000: Loss=0.19171231985092163\n",
            "Iteration 8000: Loss=0.19032752513885498\n",
            "Iteration 9000: Loss=0.18935859203338623\n",
            "Iteration 10000: Loss=0.18868963420391083\n",
            "Iteration 11000: Loss=0.18820200860500336\n",
            "Iteration 12000: Loss=0.18781974911689758\n",
            "Iteration 13000: Loss=0.18748362362384796\n",
            "Iteration 14000: Loss=0.18718299269676208\n",
            "Iteration 15000: Loss=0.18691706657409668\n",
            "Iteration 16000: Loss=0.1866825968027115\n",
            "Iteration 17000: Loss=0.18648342788219452\n",
            "Iteration 18000: Loss=0.18631915748119354\n",
            "Iteration 19000: Loss=0.1861724555492401\n",
            "Iteration 20000: Loss=0.18604491651058197\n",
            "Iteration 21000: Loss=0.18592949211597443\n",
            "Iteration 22000: Loss=0.18582521378993988\n",
            "Iteration 23000: Loss=0.1857108175754547\n",
            "Iteration 24000: Loss=0.18560072779655457\n",
            "Iteration 25000: Loss=0.18549418449401855\n",
            "Iteration 26000: Loss=0.18539367616176605\n",
            "Iteration 27000: Loss=0.1853042095899582\n",
            "Iteration 28000: Loss=0.18522201478481293\n",
            "Iteration 29000: Loss=0.18514198064804077\n",
            "Iteration 30000: Loss=0.18506434559822083\n",
            "Iteration 31000: Loss=0.1849883496761322\n",
            "Iteration 32000: Loss=0.18491043150424957\n",
            "Iteration 33000: Loss=0.18482549488544464\n",
            "Iteration 34000: Loss=0.18474772572517395\n",
            "Iteration 35000: Loss=0.18467412889003754\n",
            "Iteration 36000: Loss=0.18459272384643555\n",
            "Iteration 37000: Loss=0.18450945615768433\n",
            "Iteration 38000: Loss=0.18442806601524353\n",
            "Iteration 39000: Loss=0.18434500694274902\n",
            "Iteration 40000: Loss=0.184256911277771\n",
            "Iteration 41000: Loss=0.18417245149612427\n",
            "Iteration 42000: Loss=0.1840902715921402\n",
            "Iteration 43000: Loss=0.18400929868221283\n",
            "Iteration 44000: Loss=0.18393276631832123\n",
            "Iteration 45000: Loss=0.18384674191474915\n",
            "Iteration 46000: Loss=0.18375973403453827\n",
            "Iteration 47000: Loss=0.18367893993854523\n",
            "Iteration 48000: Loss=0.1835985779762268\n",
            "Iteration 49000: Loss=0.18352092802524567\n",
            "Iteration 50000: Loss=0.18344242870807648\n",
            "Iteration 51000: Loss=0.18333907425403595\n",
            "Iteration 52000: Loss=0.18325108289718628\n",
            "Iteration 53000: Loss=0.18316307663917542\n",
            "Iteration 54000: Loss=0.1830725371837616\n",
            "Iteration 55000: Loss=0.18298067152500153\n",
            "Iteration 56000: Loss=0.1828886866569519\n",
            "Iteration 57000: Loss=0.18279434740543365\n",
            "Iteration 58000: Loss=0.1824323982000351\n",
            "Iteration 59000: Loss=0.182278111577034\n",
            "Iteration 60000: Loss=0.1821606457233429\n",
            "Iteration 61000: Loss=0.18202081322669983\n",
            "Iteration 62000: Loss=0.18186332285404205\n",
            "Iteration 63000: Loss=0.1817130595445633\n",
            "Iteration 64000: Loss=0.18153120577335358\n",
            "Iteration 65000: Loss=0.18130910396575928\n",
            "Iteration 66000: Loss=0.18112899363040924\n",
            "Iteration 67000: Loss=0.18096093833446503\n",
            "Iteration 68000: Loss=0.180809885263443\n",
            "Iteration 69000: Loss=0.18065519630908966\n",
            "Iteration 70000: Loss=0.1804773509502411\n",
            "Iteration 71000: Loss=0.1803261786699295\n",
            "Iteration 72000: Loss=0.1801721602678299\n",
            "Iteration 73000: Loss=0.18002167344093323\n",
            "Iteration 74000: Loss=0.17987452447414398\n",
            "Iteration 75000: Loss=0.17973120510578156\n",
            "Iteration 76000: Loss=0.17958523333072662\n",
            "Iteration 77000: Loss=0.179439514875412\n",
            "Iteration 78000: Loss=0.17929138243198395\n",
            "Iteration 79000: Loss=0.17914049327373505\n",
            "Iteration 80000: Loss=0.17898884415626526\n",
            "Iteration 81000: Loss=0.1788358837366104\n",
            "Iteration 82000: Loss=0.17868177592754364\n",
            "Iteration 83000: Loss=0.17852436006069183\n",
            "Iteration 84000: Loss=0.1783631145954132\n",
            "Iteration 85000: Loss=0.17819777131080627\n",
            "Iteration 86000: Loss=0.1780298501253128\n",
            "Iteration 87000: Loss=0.1778581589460373\n",
            "Iteration 88000: Loss=0.1776829957962036\n",
            "Iteration 89000: Loss=0.177499920129776\n",
            "Iteration 90000: Loss=0.17730876803398132\n",
            "Iteration 91000: Loss=0.17711584270000458\n",
            "Iteration 92000: Loss=0.17691364884376526\n",
            "Iteration 93000: Loss=0.17670877277851105\n",
            "Iteration 94000: Loss=0.17649920284748077\n",
            "Iteration 95000: Loss=0.17627887427806854\n",
            "Iteration 96000: Loss=0.17605777084827423\n",
            "Iteration 97000: Loss=0.1758318841457367\n",
            "Iteration 98000: Loss=0.17559978365898132\n",
            "Iteration 99000: Loss=0.17536133527755737\n",
            "Accuracy: 0.908\n"
          ]
        }
      ]
    }
  ]
}